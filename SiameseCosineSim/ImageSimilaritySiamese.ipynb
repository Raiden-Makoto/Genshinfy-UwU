{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2770256,"sourceType":"datasetVersion","datasetId":1690609}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Similarity Estimation using a Siamese Network\nThe goal: given a database of anime images and an image of the user's face, return the top 5 anime characters that most closely resemble the user.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting and graphics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\n\nimport keras\nfrom keras import applications\nfrom keras.applications import resnet\nfrom keras import ops, layers\nfrom keras import optimizers, metrics\n\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTARGET_SHAPE=(256, 256)\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nautotune = tf.data.AUTOTUNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:29:49.327156Z","iopub.execute_input":"2025-02-28T03:29:49.327356Z","iopub.status.idle":"2025-02-28T03:30:01.677510Z","shell.execute_reply.started":"2025-02-28T03:29:49.327336Z","shell.execute_reply":"2025-02-28T03:30:01.676863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As a placeholder dataset, we use the **Totally Looks Like** dataset to test the capabilities of the Siamese Network. Once the **CycleGAN** is able to amass a repository of real/anime image-pairs, we can train **ISSN** on our custom dataset.","metadata":{}},{"cell_type":"code","source":"def preprocess_image(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, TARGET_SHAPE)\n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:01.678300Z","iopub.execute_input":"2025-02-28T03:30:01.678835Z","iopub.status.idle":"2025-02-28T03:30:01.682928Z","shell.execute_reply.started":"2025-02-28T03:30:01.678810Z","shell.execute_reply":"2025-02-28T03:30:01.682053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_dataset(path):\n    image_paths = tf.data.Dataset.list_files(\n        os.path.join(path, '*'),\n        shuffle=False\n    )\n    dataset = image_paths.map(\n        preprocess_image,\n        num_parallel_calls=autotune\n    )\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:01.683720Z","iopub.execute_input":"2025-02-28T03:30:01.684034Z","iopub.status.idle":"2025-02-28T03:30:01.716871Z","shell.execute_reply.started":"2025-02-28T03:30:01.684002Z","shell.execute_reply":"2025-02-28T03:30:01.716172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"AnchorImages = load_dataset('/kaggle/input/totally-looks-like-dataset/left/left')\nPositiveImages = load_dataset('/kaggle/input/totally-looks-like-dataset/right/right')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:01.717541Z","iopub.execute_input":"2025-02-28T03:30:01.717777Z","iopub.status.idle":"2025-02-28T03:30:12.646982Z","shell.execute_reply.started":"2025-02-28T03:30:01.717752Z","shell.execute_reply":"2025-02-28T03:30:12.646079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NegativeImages = tf.data.experimental.sample_from_datasets([AnchorImages, PositiveImages])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:12.647945Z","iopub.execute_input":"2025-02-28T03:30:12.648254Z","iopub.status.idle":"2025-02-28T03:30:12.672101Z","shell.execute_reply.started":"2025-02-28T03:30:12.648225Z","shell.execute_reply":"2025-02-28T03:30:12.671338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ISSNDataset = tf.data.Dataset.zip((\n    AnchorImages,\n    PositiveImages,\n    NegativeImages\n)).shuffle(buffer_size=1024)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:12.674204Z","iopub.execute_input":"2025-02-28T03:30:12.674452Z","iopub.status.idle":"2025-02-28T03:30:12.685674Z","shell.execute_reply.started":"2025-02-28T03:30:12.674429Z","shell.execute_reply":"2025-02-28T03:30:12.685020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SPLIT = round(len(AnchorImages) * 0.8)\nISSNTrain = ISSNDataset.take(SPLIT).batch(32, drop_remainder=False).prefetch(autotune)\nISSNVal = ISSNDataset.skip(SPLIT).batch(32, drop_remainder=False).prefetch(autotune)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:12.686858Z","iopub.execute_input":"2025-02-28T03:30:12.687159Z","iopub.status.idle":"2025-02-28T03:30:12.705654Z","shell.execute_reply.started":"2025-02-28T03:30:12.687138Z","shell.execute_reply":"2025-02-28T03:30:12.704966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can look at a few examples of the triplets. The Anchor and Positive images should match, while the Negative image is *always*different.","metadata":{}},{"cell_type":"code","source":"def visualize(anchor, positive, negative):\n    def show(ax, image):\n        ax.imshow(image)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n    fig = plt.figure(figsize=(9, 9))\n    axs = fig.subplots(3, 3)\n    for i in range(3):\n        show(axs[i, 0], anchor[i])\n        show(axs[i, 1], positive[i])\n        show(axs[i, 2], negative[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:12.706450Z","iopub.execute_input":"2025-02-28T03:30:12.706738Z","iopub.status.idle":"2025-02-28T03:30:12.713512Z","shell.execute_reply.started":"2025-02-28T03:30:12.706718Z","shell.execute_reply":"2025-02-28T03:30:12.712779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(\n    iter(\n        ISSNTrain.take(1).as_numpy_iterator()\n    )\n)  # Get one batch\nvisualize(*batch)  # Unpack the batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:12.714232Z","iopub.execute_input":"2025-02-28T03:30:12.714421Z","iopub.status.idle":"2025-02-28T03:30:18.102814Z","shell.execute_reply.started":"2025-02-28T03:30:12.714405Z","shell.execute_reply":"2025-02-28T03:30:18.101901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a Siamese Network from the ResNET 50 Model","metadata":{}},{"cell_type":"code","source":"ResNET50 = resnet.ResNet50(\n    weights = \"imagenet\",\n    input_shape = TARGET_SHAPE + (3,),\n    include_top = False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:18.103591Z","iopub.execute_input":"2025-02-28T03:30:18.103836Z","iopub.status.idle":"2025-02-28T03:30:26.425701Z","shell.execute_reply.started":"2025-02-28T03:30:18.103817Z","shell.execute_reply":"2025-02-28T03:30:26.424983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Siamese Network will generate embeddings for each of the images of the triplet using a **ResNet50** model pretrained on ImageNet. It will connect to a few Dense layers so we can learn to separate these embeddings.\n\nWe will freeze the weights of all the layers of the model up until the layer `conv5_block1_out`to avoid changing the weights that the model has already learned (that would be bad!). Thus, we are going to leave the bottom few layers trainable so that we can fine-tune their weights during training.","metadata":{}},{"cell_type":"code","source":"flatten = layers.Flatten()(ResNET50.output)\ndense1 = layers.Dense(512, activation=\"relu\")(flatten)\ndense1 = layers.BatchNormalization()(dense1)\ndense2 = layers.Dense(256, activation=\"relu\")(dense1)\ndense2 = layers.BatchNormalization()(dense2)\noutput = layers.Dense(256)(dense2)\n\nEmbedder = keras.Model(ResNET50.input, output, name=\"ResNET50_Embedding\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.426514Z","iopub.execute_input":"2025-02-28T03:30:26.426771Z","iopub.status.idle":"2025-02-28T03:30:26.479830Z","shell.execute_reply.started":"2025-02-28T03:30:26.426750Z","shell.execute_reply":"2025-02-28T03:30:26.478949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable = False\nfor layer in ResNET50.layers:\n    if layer.name == \"conv5_block1_out\":\n        trainable = True\n    layer.trainable = trainable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.480726Z","iopub.execute_input":"2025-02-28T03:30:26.480963Z","iopub.status.idle":"2025-02-28T03:30:26.486999Z","shell.execute_reply.started":"2025-02-28T03:30:26.480943Z","shell.execute_reply":"2025-02-28T03:30:26.486265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We also need a **Custom Layer** to compute the distance between the anchor and the positive embedding, as well as the distance between the anchor and the negative embedding.","metadata":{}},{"cell_type":"code","source":"class APNDistance(layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, anchor, positive, negative):\n        ap_distance = ops.sum(tf.square(anchor - positive), -1)\n        an_distance = ops.sum(tf.square(anchor - negative), -1)\n        return (ap_distance, an_distance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.487656Z","iopub.execute_input":"2025-02-28T03:30:26.487932Z","iopub.status.idle":"2025-02-28T03:30:26.501899Z","shell.execute_reply.started":"2025-02-28T03:30:26.487911Z","shell.execute_reply":"2025-02-28T03:30:26.501163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"anchor_input = layers.Input(name=\"anchor\", shape=TARGET_SHAPE + (3,))\npositive_input = layers.Input(name=\"positive\", shape=TARGET_SHAPE + (3,))\nnegative_input = layers.Input(name=\"negative\", shape=TARGET_SHAPE + (3,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.502643Z","iopub.execute_input":"2025-02-28T03:30:26.502866Z","iopub.status.idle":"2025-02-28T03:30:26.518476Z","shell.execute_reply.started":"2025-02-28T03:30:26.502847Z","shell.execute_reply":"2025-02-28T03:30:26.517730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Distance = APNDistance()(\n    Embedder(resnet.preprocess_input(anchor_input)),\n    Embedder(resnet.preprocess_input(positive_input)),\n    Embedder(resnet.preprocess_input(negative_input))\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.519193Z","iopub.execute_input":"2025-02-28T03:30:26.519413Z","iopub.status.idle":"2025-02-28T03:30:26.550346Z","shell.execute_reply.started":"2025-02-28T03:30:26.519394Z","shell.execute_reply":"2025-02-28T03:30:26.549771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SiameseNetworkModel = keras.Model(\n    inputs = [anchor_input, positive_input, negative_input],\n    outputs = Distance,\n    name = \"SiameseNetworkModel\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.551029Z","iopub.execute_input":"2025-02-28T03:30:26.551245Z","iopub.status.idle":"2025-02-28T03:30:26.556902Z","shell.execute_reply.started":"2025-02-28T03:30:26.551225Z","shell.execute_reply":"2025-02-28T03:30:26.556152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SiameseNetworkModel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.557520Z","iopub.execute_input":"2025-02-28T03:30:26.557748Z","iopub.status.idle":"2025-02-28T03:30:26.602481Z","shell.execute_reply.started":"2025-02-28T03:30:26.557730Z","shell.execute_reply":"2025-02-28T03:30:26.601917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the Model \nImplement a model with custom training loop so we can compute the triplet loss using the three embeddings produced by the Siamese network.","metadata":{}},{"cell_type":"code","source":"class SiameseImageSimilarityModel(keras.Model):\n    def __init__(self, actual_network, margin=.5):\n        super().__init__()\n        self.siamese_network = actual_network\n        self.margin = margin\n        self.loss_tracker = metrics.Mean(name=\"loss\")\n\n    def call(self, inputs):\n        return self.siamese_network(inputs)\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            loss = self._compute_loss(data)\n        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n        self.optimizer.apply_gradients(\n            zip(gradients, self.siamese_network.trainable_weights)\n        )\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, data):\n        loss = self._compute_loss(data)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def _compute_loss(self, data):\n        ap_distance, an_distance = self.siamese_network(data)\n        loss = ap_distance - an_distance\n        loss = tf.maximum(loss + self.margin, 0.0) #can't have negative loss\n        return loss\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.603143Z","iopub.execute_input":"2025-02-28T03:30:26.603413Z","iopub.status.idle":"2025-02-28T03:30:26.609231Z","shell.execute_reply.started":"2025-02-28T03:30:26.603394Z","shell.execute_reply":"2025-02-28T03:30:26.608609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"siamese_model = SiameseImageSimilarityModel(SiameseNetworkModel)\nsiamese_model.compile(optimizer=optimizers.Adam(0.0001))\nhistory = siamese_model.fit(ISSNTrain, epochs=15, validation_data=ISSNVal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:30:26.609995Z","iopub.execute_input":"2025-02-28T03:30:26.610231Z","iopub.status.idle":"2025-02-28T03:42:40.183557Z","shell.execute_reply.started":"2025-02-28T03:30:26.610213Z","shell.execute_reply":"2025-02-28T03:42:40.182853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = next(iter(ISSNTrain))\nvisualize(*sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:42:40.192906Z","iopub.execute_input":"2025-02-28T03:42:40.193136Z","iopub.status.idle":"2025-02-28T03:42:42.421943Z","shell.execute_reply.started":"2025-02-28T03:42:40.193119Z","shell.execute_reply":"2025-02-28T03:42:42.420914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"anchor, positive, negative = sample\nanchor_embedding, positive_embedding, negative_embedding = (\n    Embedder(resnet.preprocess_input(anchor)),\n    Embedder(resnet.preprocess_input(positive)),\n    Embedder(resnet.preprocess_input(negative)),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:42:42.423032Z","iopub.execute_input":"2025-02-28T03:42:42.423272Z","iopub.status.idle":"2025-02-28T03:42:43.806648Z","shell.execute_reply.started":"2025-02-28T03:42:42.423251Z","shell.execute_reply":"2025-02-28T03:42:43.805975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cosine_similarity = metrics.CosineSimilarity()\n\npositive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\nprint(\"Positive similarity:\", positive_similarity.numpy())\n\nnegative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\nprint(\"Negative similarity\", negative_similarity.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T03:42:43.807377Z","iopub.execute_input":"2025-02-28T03:42:43.807608Z","iopub.status.idle":"2025-02-28T03:42:44.370895Z","shell.execute_reply.started":"2025-02-28T03:42:43.807588Z","shell.execute_reply":"2025-02-28T03:42:44.370118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract data\nhistory_dict = history.history\nloss = history_dict[\"loss\"]\nval_loss = history_dict[\"val_loss\"]\naccuracy = history_dict.get(\"accuracy\")  # Works for classification models\nval_accuracy = history_dict.get(\"val_accuracy\")\n\nepochs = range(1, len(loss) + 1)\n\n# Plot Loss\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, loss, \"bo-\", label=\"Training Loss\")\nplt.plot(epochs, val_loss, \"r^-\", label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Training & Validation Loss\")\n\n# Plot Accuracy (if available)\nif accuracy:\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, \"bo-\", label=\"Training Accuracy\")\n    plt.plot(epochs, val_accuracy, \"r^-\", label=\"Validation Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.title(\"Training & Validation Accuracy\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:02:39.820034Z","iopub.execute_input":"2025-02-28T04:02:39.820351Z","iopub.status.idle":"2025-02-28T04:02:40.067005Z","shell.execute_reply.started":"2025-02-28T04:02:39.820328Z","shell.execute_reply":"2025-02-28T04:02:40.066333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = siamese_model.evaluate(ISSNVal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:11:16.316866Z","iopub.execute_input":"2025-02-28T04:11:16.317199Z","iopub.status.idle":"2025-02-28T04:11:30.752620Z","shell.execute_reply.started":"2025-02-28T04:11:16.317176Z","shell.execute_reply":"2025-02-28T04:11:30.751760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:12:42.276076Z","iopub.execute_input":"2025-02-28T04:12:42.276362Z","iopub.status.idle":"2025-02-28T04:12:42.280947Z","shell.execute_reply.started":"2025-02-28T04:12:42.276339Z","shell.execute_reply":"2025-02-28T04:12:42.280086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}